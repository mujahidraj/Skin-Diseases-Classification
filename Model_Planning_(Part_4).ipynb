{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image Augmentation"
      ],
      "metadata": {
        "id": "HUklUn5PyzKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Augmentation\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# --- Configuration ---\n",
        "# Set the path to your dataset folder in Google Drive\n",
        "base_dataset_path = '/content/drive/MyDrive/Thesis/dataset' # <-- IMPORTANT: Check this path is correct\n",
        "\n",
        "# Define the folders for each class\n",
        "class_folders = {\n",
        "    'acnes': 'acnes',\n",
        "    'hyperpigmentation': 'hyperpigmentation',\n",
        "    'Nail_psoriasis': 'Nail_psoriasis',\n",
        "    'SJS-TEN': 'SJS-TEN',\n",
        "    'Vitiligo': 'Vitiligo' # <-- Double-check this name matches your folder exactly\n",
        "}\n",
        "\n",
        "# The target number of images per class\n",
        "target_image_count = 2000\n",
        "\n",
        "# --- Augmentation Setup ---\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# --- Main Loop ---\n",
        "for class_name, folder_name in class_folders.items():\n",
        "    folder_path = os.path.join(base_dataset_path, folder_name)\n",
        "\n",
        "    # --- ‚≠êÔ∏è ERROR FIX: Check if the folder exists ---\n",
        "    if not os.path.isdir(folder_path):\n",
        "        print(f\"üö® ERROR: Directory not found for class '{class_name}': {folder_path}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Get the list of all images in the folder\n",
        "    images = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "    num_images = len(images)\n",
        "\n",
        "    print(f\"\\nProcessing '{class_name}': Found {num_images} images.\")\n",
        "\n",
        "    # --- Logic to Increase, Decrease, or Skip ---\n",
        "    if num_images < target_image_count:\n",
        "        # --- AUGMENTATION (INCREASE) LOGIC ---\n",
        "        num_to_generate = target_image_count - num_images\n",
        "        print(f\"Augmenting '{class_name}': Generating {num_to_generate} new images...\")\n",
        "\n",
        "        # Select random images from the folder to augment\n",
        "        images_to_augment = random.choices(images, k=num_to_generate)\n",
        "\n",
        "        for image_name in images_to_augment:\n",
        "            image_path = os.path.join(folder_path, image_name)\n",
        "            try:\n",
        "                img = Image.open(image_path).convert('RGB')\n",
        "                img = img.resize((224, 224))\n",
        "                x = np.array(img)\n",
        "                x = x.reshape((1,) + x.shape)\n",
        "\n",
        "                # Generate one augmented image\n",
        "                for batch in datagen.flow(x, batch_size=1):\n",
        "                    augmented_image = Image.fromarray(batch[0].astype('uint8'))\n",
        "                    new_image_name = f\"aug_{random.randint(10000, 99999)}_{image_name}\"\n",
        "                    augmented_image.save(os.path.join(folder_path, new_image_name))\n",
        "                    break # Stop after one is generated\n",
        "            except Exception as e:\n",
        "                print(f\"Could not process image {image_name}. Error: {e}\")\n",
        "\n",
        "    elif num_images > target_image_count:\n",
        "        # --- ‚≠êÔ∏è NEW: DECREASE LOGIC ---\n",
        "        num_to_delete = num_images - target_image_count\n",
        "        print(f\"Decreasing '{class_name}': Randomly deleting {num_to_delete} images...\")\n",
        "\n",
        "        # Randomly select images to delete\n",
        "        images_to_delete = random.sample(images, num_to_delete)\n",
        "\n",
        "        for image_name in images_to_delete:\n",
        "            file_to_delete_path = os.path.join(folder_path, image_name)\n",
        "            try:\n",
        "                os.remove(file_to_delete_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Could not delete image {image_name}. Error: {e}\")\n",
        "\n",
        "    else: # num_images == target_image_count\n",
        "        print(f\"'{class_name}' already has {target_image_count} images. No action needed.\")\n",
        "\n",
        "\n",
        "print(\"\\nProcessing complete! ‚úÖ\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4jo8dXHy2VV",
        "outputId": "ea1ca07e-4baa-44c0-f232-d76af6dfed58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing 'acnes': Found 1148 images.\n",
            "Augmenting 'acnes': Generating 852 new images...\n",
            "\n",
            "Processing 'hyperpigmentation': Found 700 images.\n",
            "Augmenting 'hyperpigmentation': Generating 1300 new images...\n",
            "\n",
            "Processing 'Nail_psoriasis': Found 2520 images.\n",
            "Decreasing 'Nail_psoriasis': Randomly deleting 520 images...\n",
            "\n",
            "Processing 'SJS-TEN': Found 3164 images.\n",
            "Decreasing 'SJS-TEN': Randomly deleting 1164 images...\n",
            "\n",
            "Processing 'Vitiligo': Found 2016 images.\n",
            "Decreasing 'Vitiligo': Randomly deleting 16 images...\n",
            "\n",
            "Processing complete! ‚úÖ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Dataset"
      ],
      "metadata": {
        "id": "M_KQ8uy38yuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# --- Configuration ---\n",
        "# Path to your balanced dataset (where each class has 2000 images)\n",
        "source_dataset_path = '/content/drive/MyDrive/Thesis/dataset' # <-- IMPORTANT: Path to your processed dataset\n",
        "\n",
        "# Path where the new split dataset will be created\n",
        "output_dataset_path = '/content/drive/MyDrive/Thesis/Split_Dataset'\n",
        "\n",
        "# Define the split ratios\n",
        "# 70% for training, 15% for validation, 15% for testing\n",
        "split_ratios = (0.7, 0.15, 0.15)\n",
        "\n",
        "# --- Main Splitting Logic ---\n",
        "print(\"Starting dataset split...\")\n",
        "\n",
        "# Create the main output directories (train, validation, test)\n",
        "if os.path.exists(output_dataset_path):\n",
        "    shutil.rmtree(output_dataset_path) # Remove old directory to ensure a clean split\n",
        "    print(f\"Removed existing directory at: {output_dataset_path}\")\n",
        "\n",
        "train_dir = os.path.join(output_dataset_path, 'train')\n",
        "validation_dir = os.path.join(output_dataset_path, 'validation')\n",
        "test_dir = os.path.join(output_dataset_path, 'test')\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Get the list of class folders from the source directory\n",
        "class_names = [d for d in os.listdir(source_dataset_path) if os.path.isdir(os.path.join(source_dataset_path, d))]\n",
        "\n",
        "# Loop through each class folder\n",
        "for class_name in class_names:\n",
        "    print(f\"\\nProcessing class: {class_name}\")\n",
        "\n",
        "    # Create class-specific subdirectories in train, validation, and test folders\n",
        "    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
        "    os.makedirs(os.path.join(validation_dir, class_name), exist_ok=True)\n",
        "    os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
        "\n",
        "    # Get all image file names for the current class\n",
        "    source_class_dir = os.path.join(source_dataset_path, class_name)\n",
        "    all_files = [f for f in os.listdir(source_class_dir) if os.path.isfile(os.path.join(source_class_dir, f))]\n",
        "\n",
        "    # Shuffle the files randomly for an unbiased split\n",
        "    random.shuffle(all_files)\n",
        "\n",
        "    # Calculate split indices\n",
        "    total_files = len(all_files)\n",
        "    train_end = int(total_files * split_ratios[0])\n",
        "    validation_end = train_end + int(total_files * split_ratios[1])\n",
        "\n",
        "    # Slice the list of files into the three sets\n",
        "    train_files = all_files[:train_end]\n",
        "    validation_files = all_files[train_end:validation_end]\n",
        "    test_files = all_files[validation_end:]\n",
        "\n",
        "    # --- Function to copy files ---\n",
        "    def copy_files(files, source_dir, dest_dir):\n",
        "        for f in files:\n",
        "            shutil.copy(os.path.join(source_dir, f), os.path.join(dest_dir, f))\n",
        "\n",
        "    # Copy files to their new destination\n",
        "    copy_files(train_files, source_class_dir, os.path.join(train_dir, class_name))\n",
        "    copy_files(validation_files, source_class_dir, os.path.join(validation_dir, class_name))\n",
        "    copy_files(test_files, source_class_dir, os.path.join(test_dir, class_name))\n",
        "\n",
        "    print(f\"  - Training set:   {len(train_files)} images\")\n",
        "    print(f\"  - Validation set: {len(validation_files)} images\")\n",
        "    print(f\"  - Testing set:    {len(test_files)} images\")\n",
        "\n",
        "print(\"\\nDataset splitting complete! ‚úÖ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y4qoWl381Hu",
        "outputId": "c251dc57-f77e-4919-fb4b-a4d857f5134d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting dataset split...\n",
            "\n",
            "Processing class: hyperpigmentation\n",
            "  - Training set:   1400 images\n",
            "  - Validation set: 300 images\n",
            "  - Testing set:    300 images\n",
            "\n",
            "Processing class: Nail_psoriasis\n",
            "  - Training set:   1400 images\n",
            "  - Validation set: 300 images\n",
            "  - Testing set:    300 images\n",
            "\n",
            "Processing class: SJS-TEN\n",
            "  - Training set:   1400 images\n",
            "  - Validation set: 300 images\n",
            "  - Testing set:    300 images\n",
            "\n",
            "Processing class: Vitiligo\n",
            "  - Training set:   1400 images\n",
            "  - Validation set: 300 images\n",
            "  - Testing set:    300 images\n",
            "\n",
            "Processing class: acnes\n",
            "  - Training set:   1400 images\n",
            "  - Validation set: 300 images\n",
            "  - Testing set:    300 images\n",
            "\n",
            "Dataset splitting complete! ‚úÖ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "hJBLJxB-_hjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## DenseNet 121"
      ],
      "metadata": {
        "id": "DgnKF7Vc_k1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "# --- 1. SETUP FOR SPEED: MIXED PRECISION ---\n",
        "\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# --- 2. Configuration and Setup ---\n",
        "\n",
        "# Define paths to your split dataset\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/Thesis/Split_Dataset'\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# Model parameters\n",
        "\n",
        "IMG_HEIGHT = 224\n",
        "\n",
        "IMG_WIDTH = 224\n",
        "\n",
        "BATCH_SIZE = 64 # Larger batch size for faster training\n",
        "\n",
        "INITIAL_EPOCHS = 10\n",
        "\n",
        "FINE_TUNE_EPOCHS = 10\n",
        "\n",
        "TOTAL_EPOCHS = INITIAL_EPOCHS + FINE_TUNE_EPOCHS\n",
        "\n",
        "# --- 3. Data Loading and Preprocessing ---\n",
        "\n",
        "# Create ImageDataGenerators with DenseNet-specific preprocessing\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "\n",
        "    preprocessing_function=tf.keras.applications.densenet.preprocess_input,\n",
        "\n",
        "    rotation_range=40,\n",
        "\n",
        "    width_shift_range=0.2,\n",
        "\n",
        "    height_shift_range=0.2,\n",
        "\n",
        "    shear_range=0.2,\n",
        "\n",
        "    zoom_range=0.2,\n",
        "\n",
        "    horizontal_flip=True,\n",
        "\n",
        "    fill_mode='nearest'\n",
        "\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.densenet.preprocess_input)\n",
        "\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.densenet.preprocess_input)\n",
        "\n",
        "# Create data generators from directories\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "\n",
        "    train_dir, target_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=BATCH_SIZE, class_mode='sparse')\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "\n",
        "    validation_dir, target_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=BATCH_SIZE, class_mode='sparse')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "\n",
        "    test_dir, target_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=BATCH_SIZE, class_mode='sparse', shuffle=False)\n",
        "\n",
        "num_classes = train_generator.num_classes\n",
        "\n",
        "print(f\"Found {num_classes} classes.\")\n",
        "\n",
        "# --- 4. Model Building (Transfer Learning with DenseNet121) ---\n",
        "\n",
        "# Load the DenseNet121 base model\n",
        "\n",
        "base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "\n",
        "# Freeze the base model\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create our custom classifier head\n",
        "\n",
        "x = base_model.output\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# Set the final layer's dtype to float32 for numerical stability\n",
        "\n",
        "predictions = Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
        "\n",
        "# Combine the base model and our custom head\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# --- 5. Compile and Train (Feature Extraction) ---\n",
        "\n",
        "# Wrap the optimizer in a LossScaleOptimizer for mixed precision\n",
        "\n",
        "optimizer = tf.keras.mixed_precision.LossScaleOptimizer(Adam(learning_rate=0.001))\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"\\n--- Starting Initial Training (Feature Extraction) ---\")\n",
        "\n",
        "history = model.fit(\n",
        "\n",
        "    train_generator,\n",
        "\n",
        "    epochs=INITIAL_EPOCHS,\n",
        "\n",
        "    validation_data=validation_generator\n",
        "\n",
        ")\n",
        "\n",
        "# --- 6. Fine-Tuning Phase ---\n",
        "\n",
        "print(\"\\n--- Starting Fine-Tuning ---\")\n",
        "\n",
        "base_model.trainable = True\n",
        "\n",
        "# DenseNet121 has 427 layers. Let's unfreeze from the last dense block onwards.\n",
        "\n",
        "fine_tune_at = 313\n",
        "\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "\n",
        "    layer.trainable = False\n",
        "\n",
        "# Re-compile with the LossScaleOptimizer for fine-tuning\n",
        "\n",
        "optimizer_fine = tf.keras.mixed_precision.LossScaleOptimizer(Adam(learning_rate=1e-5))\n",
        "\n",
        "model.compile(optimizer=optimizer_fine,\n",
        "\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history_fine = model.fit(\n",
        "\n",
        "    train_generator,\n",
        "\n",
        "    epochs=TOTAL_EPOCHS,\n",
        "\n",
        "    initial_epoch=history.epoch[-1],\n",
        "\n",
        "    validation_data=validation_generator\n",
        "\n",
        ")\n",
        "\n",
        "# --- 7. Final Evaluation ---\n",
        "\n",
        "print(\"\\n--- Evaluating Model on Test Data ---\")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# --- 8. Plotting History ---\n",
        "\n",
        "acc = history.history['accuracy'] + history_fine.history['accuracy']\n",
        "\n",
        "val_acc = history.history['val_accuracy'] + history_fine.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss'] + history_fine.history['loss']\n",
        "\n",
        "val_loss = history.history['val_loss'] + history_fine.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "\n",
        "plt.axvline(INITIAL_EPOCHS - 1, color='gray', linestyle='--', label='Start Fine-Tuning')\n",
        "\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "\n",
        "plt.plot(loss, label='Training Loss')\n",
        "\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "\n",
        "plt.axvline(INITIAL_EPOCHS - 1, color='gray', linestyle='--', label='Start Fine-Tuning')\n",
        "\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "Rlc31KiIHWwz",
        "outputId": "d442f6dd-559e-4669-8987-9ebb8e8c8442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7000 images belonging to 5 classes.\n",
            "Found 1500 images belonging to 5 classes.\n",
            "Found 1500 images belonging to 5 classes.\n",
            "Found 5 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m29084464/29084464\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "--- Starting Initial Training (Feature Extraction) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m  4/110\u001b[0m \u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m34:45\u001b[0m 20s/step - accuracy: 0.1549 - loss: 2.7844"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3643864515.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Starting Initial Training (Feature Extraction) ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix**"
      ],
      "metadata": {
        "id": "RwNnzoIxb9wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# --- 1. Get Predictions and True Labels ---\n",
        "\n",
        "# Reset the test generator to ensure the order is correct\n",
        "\n",
        "test_generator.reset()\n",
        "\n",
        "# Use the trained model to make predictions on the test set\n",
        "\n",
        "y_pred_probs = model.predict(test_generator, steps=int(np.ceil(test_generator.samples / test_generator.batch_size)))\n",
        "\n",
        "# Convert prediction probabilities to class indices\n",
        "\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Get the ground truth labels from the generator\n",
        "\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Get the names of the classes for labeling\n",
        "\n",
        "class_names = list(test_generator.class_indices.keys())\n",
        "\n",
        "\n",
        "# --- 2. Generate Classification Report ---\n",
        "\n",
        "print(\"\\n## Classification Report ##\\n\")\n",
        "\n",
        "# This report shows precision, recall, f1-score, and support for each class\n",
        "\n",
        "report = classification_report(y_true, y_pred, target_names=class_names)\n",
        "\n",
        "print(report)\n",
        "\n",
        "\n",
        "# --- 3. Plot Confusion Matrix ---\n",
        "\n",
        "print(\"\\n## Confusion Matrix ##\\n\")\n",
        "\n",
        "# Create the confusion matrix\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using a heatmap\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges',\n",
        "\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "plt.ylabel('True Label')\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "0DNqXz1g0h0P",
        "outputId": "9db83284-c5cf-4954-8aa4-fb1211dc0847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_generator' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2146970185.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Reset the test generator to ensure the order is correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtest_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Use the trained model to make predictions on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_generator' is not defined"
          ]
        }
      ]
    }
  ]
}